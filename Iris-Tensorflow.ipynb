{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de uso de tensorflow para predecir el dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lectura de datos\n",
    "1. Carga de datos.\n",
    "2. Codificaci칩n de target y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "labelBinarizer = LabelBinarizer()\n",
    "\n",
    "dataset = pd.DataFrame(data.data, columns=['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "dataset['specie'] = data.target\n",
    "Y = labelBinarizer.fit_transform(dataset.specie.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizaci칩n de los datos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.columns[0:4]\n",
    "X_data = dataset[X].values\n",
    "X_data = normalize(X_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividir los datos en entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construcci칩n de la red neural con tensorflow\n",
    "\n",
    "La red neuronal propuesta tendr치 dos capas de 256 y 128 neuronas respectivamente y una capa de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "training_epochs = 3000\n",
    "n_hidden1 = 25  # Number of neurons in layer 1\n",
    "#n_hidden2 = 128 # Number of neurons in layer 2\n",
    "n_input = X_train.shape[1] #Number of features (4)\n",
    "n_classes = y_train.shape[1] #Number of classes to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None,n_input], dtype = tf.float32)\n",
    "y = tf.placeholder(shape=[None,n_classes], dtype = tf.float32)\n",
    "\n",
    "#Weights\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden1, n_classes]))\n",
    "}\n",
    "\n",
    "#Biases\n",
    "biases = {\n",
    "  'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "  'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x):\n",
    "    #Hidden layer 1.\n",
    "    out_h1 = tf.nn.relu(tf.matmul(x,weights['h1'])+biases['b1'])    \n",
    "    #Output layer\n",
    "    out_layer = tf.matmul(out_h1, weights['out'])+biases['out']\n",
    "    \n",
    "    return out_layer\n",
    "\n",
    "y_hat = forward_propagation(X)\n",
    "y_predict = tf.argmax(y_hat, axis = 1) #Devuelve la columna con el mayor valor.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits =y_hat))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op= optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71066905 0.35533453 0.56853524 0.21320072]\n",
      " [0.72415258 0.32534391 0.56672811 0.22039426]\n",
      " [0.69997037 0.32386689 0.58504986 0.25073566]\n",
      " [0.73337886 0.32948905 0.54206264 0.24445962]\n",
      " [0.69052512 0.32145135 0.60718588 0.22620651]\n",
      " [0.69193502 0.32561648 0.60035539 0.23403685]\n",
      " [0.68914871 0.33943145 0.58629069 0.25714504]\n",
      " [0.72155725 0.32308533 0.56001458 0.24769876]\n",
      " [0.72965359 0.28954508 0.57909015 0.22005426]\n",
      " [0.71653899 0.3307103  0.57323119 0.22047353]\n",
      " [0.67467072 0.36998072 0.58761643 0.25028107]\n",
      " [0.69025916 0.35097923 0.5966647  0.21058754]]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "    arrays = np.array([[0.71066905, 0.35533453, 0.56853524, 0.21320072],\n",
    "       [0.72415258, 0.32534391, 0.56672811, 0.22039426],\n",
    "       [0.69997037, 0.32386689, 0.58504986, 0.25073566],\n",
    "       [0.73337886, 0.32948905, 0.54206264, 0.24445962],\n",
    "       [0.69052512, 0.32145135, 0.60718588, 0.22620651],\n",
    "       [0.69193502, 0.32561648, 0.60035539, 0.23403685],\n",
    "       [0.68914871, 0.33943145, 0.58629069, 0.25714504],\n",
    "       [0.72155725, 0.32308533, 0.56001458, 0.24769876],\n",
    "       [0.72965359, 0.28954508, 0.57909015, 0.22005426],\n",
    "       [0.71653899, 0.3307103 , 0.57323119, 0.22047353],\n",
    "       [0.67467072, 0.36998072, 0.58761643, 0.25028107],\n",
    "       [0.69025916, 0.35097923, 0.5966647 , 0.21058754]])\n",
    "    \n",
    "    print(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1000, train accuracy = 76.79%, test accuracy = 78.95%\n",
      "Epoch = 2000, train accuracy = 89.29%, test accuracy = 84.21%\n",
      "Epoch = 3000, train accuracy = 95.54%, test accuracy = 86.84%\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "Total time: 7.674698114395142\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = 120\n",
    "loss = []\n",
    "start = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #writer.add_graph(sess.graph)\n",
    "    #EPOCHS\n",
    "    for epoch in range(training_epochs):\n",
    "        size = np.random.choice(len(X_train),batch_size)\n",
    "\n",
    "        x_batch = X_train[size]\n",
    "        y_batch = y_train[size]\n",
    "        #Stochasting Gradient Descent\n",
    "        #for i in range(len(X_train)):\n",
    "        #    summary = sess.run(train_op, feed_dict={X: X_train[i:i+1], y: y_train[i:i+1]})\n",
    "        #for i in range(len(X_train)):\n",
    "        summary = sess.run(train_op, feed_dict={X: x_batch, y: y_batch})\n",
    "        \n",
    "        train_accuracy = np.mean(np.argmax(y_train, axis=1) == sess.run(y_predict, feed_dict={X: X_train, y: y_train}))\n",
    "        test_accuracy  = np.mean(np.argmax(y_test, axis=1) == sess.run(y_predict, feed_dict={X: X_test, y: y_test}))\n",
    "        loss.append(sess.run(cost, feed_dict={X: X_train, y: y_train}))\n",
    "        if ((epoch +1 ) % 1000 == 0):\n",
    "            print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\" % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "\n",
    "    \n",
    "   # print(sess.run(forward_propagation([[1.,1.,1.,1.]])))\n",
    "    for array in arrays:\n",
    "        print(sess.run(tf.argmax(forward_propagation(np.float32(array.reshape(1,4))),axis=1)))\n",
    "    sess.close()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Total time: {0}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(range(1,5001)),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(range(1,201)),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "print(forward_propagation([[1.,1.,1.,1.]]))\n",
    "#session.run(forward_propagation([[1.,1.,1.,1.]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
